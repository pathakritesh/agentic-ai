# ğŸ“„ Local RAG Agent (Ollama + LlamaIndex + Chroma)

A **fully local Retrieval-Augmented Generation (RAG) system** that allows you to **chat with multiple PDF files** using **Ollama**, **LlamaIndex**, **ChromaDB**, **FastAPI**, and **Streamlit**.

This project is optimized to run on a **Windows laptop with 8 GB RAM**, without any external API keys (no OpenAI, no cloud dependency).

---

## âœ¨ Features

- ğŸ“š Multi-PDF ingestion
- ğŸ” Semantic search using vector embeddings
- ğŸ’¬ Conversational chat interface
- ğŸ§  Local LLM inference via Ollama
- ğŸ’¾ Persistent vector storage (ChromaDB)
- ğŸ“Œ Source citations (PDF file name + page number)
- ğŸš€ FastAPI backend + Streamlit UI
- ğŸ”’ Fully offline & privacy-friendly

---

## ğŸ§± Architecture Overview

PDF Files
â†“
LlamaIndex (Loader + Chunking)
â†“
Ollama Embeddings (nomic-embed-text)
â†“
ChromaDB (Persistent Vector Store)
â†“
FastAPI (Query API)
â†“
Streamlit (Chat UI)
â†“
Ollama LLM (llama3.2:3b)


---

## ğŸ“ Repository Structure

Local-RAG-Agent-V2/
â”‚
â”œâ”€â”€ Local-RAG-Agent-V2.ipynb # Jupyter notebook (experimentation)
â”œâ”€â”€ app.py # FastAPI backend
â”œâ”€â”€ streamlit_app.py # Streamlit chat UI
â”œâ”€â”€ pyproject.toml # Dependencies
â”œâ”€â”€ chroma_db/ # Persistent vector database
â””â”€â”€ data/
â””â”€â”€ pdfs/ # All PDF files go here


---

## âš™ï¸ Prerequisites

### 1ï¸âƒ£ System Requirements
- OS: Windows / Linux / macOS
- RAM: **8 GB minimum**
- Python: **3.10+**
- Disk: ~5 GB free

---

3ï¸âƒ£ Pull Required Models
ollama pull nomic-embed-text
ollama pull llama3.2:3b


Start Ollama:

ollama serve

ğŸ Python Environment Setup
Create & activate virtual environment (Windows)
python -m venv .venv
.\.venv\Scripts\activate

Install dependencies
pip install -r requirements.txt


If using pyproject.toml:

pip install .

ğŸ“„ Add PDF Files

Place all PDFs inside:

data/pdfs/


You can add 20+ PDFs for testing.

ğŸš€ Running the Application
1ï¸âƒ£ Start FastAPI Backend
uvicorn app:app --host 127.0.0.1 --port 8000 --reload


Verify backend is running:

Open: http://127.0.0.1:8000/docs

2ï¸âƒ£ Start Streamlit UI (New Terminal)
streamlit run streamlit_app.py


Streamlit will open automatically in your browser.

ğŸ’¬ Usage

Type a question in the chat input

The system:

Searches relevant PDF chunks

Generates an answer using local LLM

Displays citations (PDF name + page number)

Example questions:

â€œSummarize the documentâ€

â€œWhich PDF talks about cost optimization?â€

â€œExplain Kubernetes networkingâ€

ğŸ“Œ Example API Response
{
  "answer": "The document explains cloud architecture best practices...",
  "sources": [
    {
      "file_name": "01_AWS_Architecture_Best_Practices.pdf",
      "page": "2"
    }
  ]
}

ğŸ§ª Jupyter Notebook

Local-RAG-Agent-V2.ipynb is included for:

Experimentation

Debugging

Learning LlamaIndex internals

âš ï¸ Production usage should rely on FastAPI + Streamlit, not the notebook.

ğŸ§  Performance Notes (8 GB RAM)

Uses llama3.2:3b (small, stable model)

Context limited to 2048 tokens

Retrieval limited to top-1 chunk

Response mode set to compact

This avoids memory crashes on low-RAM systems.

ğŸ› ï¸ Troubleshooting
âŒ Streamlit Connection Error
Connection refused (127.0.0.1:8000)


âœ… Fix:

Ensure FastAPI is running

Check http://127.0.0.1:8000/docs

Start FastAPI before Streamlit

Ensure same virtual environment is used
